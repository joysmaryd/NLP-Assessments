{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9274d42a-3300-48a6-8566-a5eba451a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python program to load a text file, perform tokenization, calculate the term\n",
    "# frequency (TF) of each token, and display the top 5 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c780c1-33b3-4eb6-8d46-50f96524a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\joysm\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\joysm\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joysm\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joysm\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joysm\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\joysm\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d520cdc-2a41-476a-8d1d-5aecdaeef4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joysm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526be2ce-0080-4f5a-a31b-73b2175a3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db43dc7e-f565-48a5-9a8c-c6e80cdb7a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joysm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0860e357-2ac2-47b9-8d14-2d8f8a9eda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text from a file\n",
    "file_path = \"sample.txt\"  # Ensure this file exists\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Creating a sample file...\")\n",
    "    text = \"Natural Language Processing is fascinating. NLP and Machine Learning are widely used in AI applications.\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29647be-f3d7-4396-8b6c-a21158ceacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "\n",
    "# Calculate Term Frequency (TF)\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "# Get the top 5 most frequent tokens\n",
    "most_common_tokens = token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ac2f79-7b5e-41a6-98f2-3b8b7f7c5bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Most Frequent Tokens:\n",
      "this: 1\n",
      "is: 1\n",
      "a: 1\n",
      "sample: 1\n",
      "text: 1\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"\\nTop 5 Most Frequent Tokens:\")\n",
    "for word, count in most_common_tokens:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16b1939f-ac7c-4bd9-9b10-df60203d5a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Most Frequent Tokens (After Stopword Removal):\n",
      "sample: 1\n",
      "text: 1\n",
      "file: 1\n",
      "preprocessing: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joysm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joysm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load text from file\n",
    "file_path = \"sample.txt\"\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Creating a sample file...\")\n",
    "    text = \"Natural Language Processing is fascinating. NLP and Machine Learning are widely used in AI applications.\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "\n",
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Calculate term frequency (TF)\n",
    "token_counts = Counter(filtered_tokens)\n",
    "\n",
    "# Get the top 5 most frequent tokens\n",
    "most_common_tokens = token_counts.most_common(5)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTop 5 Most Frequent Tokens (After Stopword Removal):\")\n",
    "for word, count in most_common_tokens:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f41e6-fc01-494f-bad2-b25f7aacd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a larger sample text\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field of AI.\n",
    "NLP helps computers understand human language.\n",
    "Machine learning models are widely used in NLP applications.\n",
    "Deep learning has improved NLP accuracy significantly.\n",
    "NLP is used in chatbots, search engines, and text analytics.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "\n",
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Calculate term frequency (TF)\n",
    "token_counts = Counter(filtered_tokens)\n",
    "\n",
    "# Get the top 5 most frequent tokens\n",
    "most_common_tokens = token_counts.most_common(5)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTop 5 Most Frequent Tokens (After Stopword Removal):\")\n",
    "for word, count in most_common_tokens:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
